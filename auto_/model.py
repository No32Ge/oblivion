import json
import time

import openai
from sympy.physics.units import sr

from auto_.call.call_manager import CallManager
from auto_.config.API import MODEL_CONFIGS
from auto_.config.prompt_config import TOOL, PromptManager, get_code_dir
from auto_.utils.chat_manager import EnhancedChatManager
from auto_.core.evolution import EvolutionEngine
from auto_.core.deepmind_core import CoreFunctionality, CoreAPI
from test.è¯­éŸ³è½¬æ–‡æœ¬.stt import  get_vosk_mic_transcriber_instance


class ConversationManager:
    def __init__(self, model_name="glm-4-plus"):
        # åŠ è½½æ¨¡å‹é…ç½®
        self.model_config = MODEL_CONFIGS.get(model_name)
        if not self.model_config:
            raise ValueError(f"Unsupported model: {model_name}")

        # åˆå§‹åŒ–æ‰€éœ€çš„æ¨¡å—å’Œå·¥å…·
        self.core = CoreFunctionality()
        self.api = CoreAPI(self.core)
        self.client = openai.OpenAI(
            api_key=self.model_config["API_KEY"],
            base_url=self.model_config["BASE_URL"]
        )
        self.call_manager = CallManager()
        self.chat_manager = EnhancedChatManager()
        self.evolution_engine = EvolutionEngine()

    def run_conversation(self, user_input):
        """æ‰§è¡Œå¯¹è¯æµç¨‹"""
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.chat_manager.add_message("user", user_input)

        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        prompt_manager = PromptManager(self.chat_manager)

        # ä¸ OpenAI è¿›è¡Œå¯¹è¯ï¼Œè·å–å“åº”
        response = self.client.chat.completions.create(
            model=self.model_config["MODEL_NAME"],
            messages=prompt_manager.get_message_prompt(),
            temperature=0.7,
            max_tokens=8192,
            tools=TOOL,
            tool_choice="auto"
        )

        # print("messageï¼š" + str(response.choices[0].message))

        # å¤„ç†å·¥å…·è°ƒç”¨
        tool_calls = response.choices[0].message.tool_calls
        if tool_calls:

            for call in tool_calls:
                tool_name = call.function.name
                arguments = json.loads(call.function.arguments)
                result = self.call_manager.execute_tool(tool_name, arguments)
                self.chat_manager.add_tool_call(tool_calls, result=result)
            time.sleep(5)
            # ç¬¬äºŒæ¬¡æŠŠ tool ç»“æœç»™ assistant çœ‹
            follow_up_response = self.client.chat.completions.create(
                model=self.model_config["MODEL_NAME"],
                messages=prompt_manager.get_message_prompt(),
                temperature=0.7,
                max_tokens=8192,
                tools=TOOL,
                tool_choice="auto"
            )
            final_result = follow_up_response.choices[0].message.content
            self.chat_manager.add_message("assistant", final_result)
            self.chat_manager.save_conversation()

            return final_result

        # ç›´æ¥å¤„ç†å“åº”
        result = response.choices[0].message.content
        self.chat_manager.save_response(result)
        self.chat_manager.add_message("assistant", result)
        self.chat_manager.save_conversation()
        return result



# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»º ConversationManager å®ä¾‹ï¼ŒæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ "glm-4-plus"ï¼‰
    # deepseek
    # transcriber =  get_vosk_mic_transcriber_instance()
    conversation_manager = ConversationManager(model_name="glm-4-plus")
    while True:
        user_input = input("è¾“å…¥å†…å®¹")
        if user_input.lower() == 'q':
            break

        response = conversation_manager.run_conversation(user_input)

        print(f"\nğŸ¤– è™šæ‹Ÿé—å¿˜:\n{response}")
